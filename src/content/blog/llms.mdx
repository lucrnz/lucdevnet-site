---
title: "AI and Large Language Models"
datePublished: "2025-05-21"
summary: "Let's talk about LLM models, their ethics, implications and the usage I give to it"
tags: "ai,llm"
---

import { Image } from 'astro:assets';
import companiesTrainingDataChart from "~/assets/blog/llms/companies-training-data-chart.svg";

## Disclaimer
**I am not an expert on this topic**; I merely know the basics of it. However, I have some information that I think can be useful for you to understand how it works and not see it as an *all-knowing computer that is plotting to rule the world*. **Do your own research.**

I also do not endorse the use of AI for content farms, spam, scamming, or other unethical use cases. AI-generated art **cannot replace artists** who put care and passion into their work, and the same goes for AI in software development and writing.

## Large Language Models - What are they?

Whether you love them or hate them, AI *or rather, large language models, which is a more accurate term* is here to annoy us or to help us.

At its core, an LLM is pretty much an auto-complete algorithm; it uses complex math to determine which words or sentences are most likely to follow a given input. Think of it like the predictive text on your phone, but instead of predicting the next word, it goes on to write entire sentences or paragraphs.

In simple terms: Someone puts a bunch of text into a computer, converts it into a numerical format, and uses it to train the model. This process is usually ultra expensive and requires an absurd amount of data. I will come back to where this data comes from later and explore the implications of it.

The model transforms whatever input you provide into a **completion** of what's next *or what it feels should be next* for that input.

Let's see a simple example: "Hey, how are you?" will most likely output "I'm good, thanks! How about you?".

Of course, there are more details to consider, such as the fact that the input is usually given in a turn-based conversation or chat. This allows the model to maintain a sense of context and respond accordingly.

Unfortunately, because it does predict stuff, its responses are not always 100% correct; this is known as hallucinations. Hallucinations happen when the model generates text that is not based on actual facts or context, but rather on patterns it learned from its training data.

## Where does the data come from?

Now in the topic of training data, let's explore where companies that train AI gather their data. The answer is simple: **everywhere they can put their hands on**, especially on the internet, if a website is public, it will definitely be crawled.

From within their products too, for example, Meta has their platform ecosystem, including Instagram, Facebook, and WhatsApp, which likely contribute to the training data for their flagship model, Llama [*I haven't fact-checked this*](https://duckduckgo.com/?q=Does+meta+train+Llama+on+users+data). OpenAI also trains data from ChatGPT users.

However, the most annoying thing is when companies literally [DDOS small communities' websites](https://lunnova.dev/articles/crawlers-please-stop-destroying-the-commons/) for constantly checking if there are new data. They visit the internet, take everything for granted, and revisit the sites every couple of milliseconds, just to check if anything changes. This is devastating for small internet authors and sysadmins.

<Image src={companiesTrainingDataChart} alt="Diagram showing multiple companies as rectangles fetching data from multiple sources, including personal blogs, Wikipedia, Reddit, video game fan-made wikis, forums, Stack Overflow, and developer documentation sites." />

As a result, many websites, including my own, have started using bot detection software *(shoutout to [the open-source program Anubis](https://anubis.techaro.lol/)  for making this accessible)* to mitigate the effects of these aggresive crawlers.

If you've visited any website in the last few months, you might have noticed these programs or "Checking if you are not a robot" screens in general.

Then there are the copyright implications, which are really complex and I don't understand them much. However, it's clear that companies training AI models are often pushing the limits of what's considered acceptable when it comes to using copyrighted material, if the data is there, they use it, and then [deploy their lawyers](https://www.saverilawfirm.com/our-cases/github-copilot-intellectual-property-litigation) to do [whatever is necesary](https://www.deeplearning.ai/the-batch/judge-dismisses-key-arguments-in-ai-copyright-lawsuit-against-github-microsoft-and-openai/).

## How I use it

I use LLMs as an assistant and information seeker, acting as a complementary tool alongside search engines.

When working on software projects, it helps me draft scripts or code snippets, particularly in languages where I struggle the most, *like Bash*. It has also been helpful for understanding complex code, optimization advice, and sometimes debugging. My experience with it is mixed: it's sometimes a complete waste of time, but other times it gives me something close to or even exactly what I need.

When writing blog posts, documentation, or emails, I use it to provide simple suggestions and help with grammatical and syntax corrections.

In my free time, I have found that it is useless for providing help in video games ðŸ¥², giving the spotlight to fan-made wiki pages and search engine queries ending with 'reddit'.

A more personal use case I found is making it role-play as a therapist. Of course this can sound like terrible advice, *and mind you it is*, so if you need help **please contact a real human professional**. In my personal experience, it helped me navigate issues in my life and understand them better.

## In the topic of "Vibe coding"

I really hate this topic, if you don't know what this means, it means [authoring software without reading the code](https://en.wikipedia.org/wiki/Vibe_coding) it generates and just keeping asking the AI to fix the bugs.

Honestly, I don't think this should be celebrated or cherished, at least in the context of authoring products that are being sold or are mission critical.

I understand and I enjoy seeing the results of experimenting with prompting, however, it is far from being that useful, and again, it cannot replace hand-crafted coding.

Be free to experiment with it and use its results as a draft, that is for sure.

## Conclusion

I do not wish for AI models to go away; however, I really wish companies would try better ways of gathering data. We need some sort of opt-in mechanism.

At the end of the day, I think that open-source models, *especially those run locally or on private infrastructure*, **can really empower users** in their needs.

> **Correction**: So called [open-source models aren't really open source sometimes](https://opensource.org/blog/metas-llama-2-license-is-not-open-source), but I mean those that can be easily downloaded compared to provider specific models such as GPT.

However, be sure to approach it with a critical eye, otherwise you might start hallucinating too ðŸ˜‰

Shoutout to Arch for writing [the article that inspired me](https://arch.dog/bark/large-language-models).
